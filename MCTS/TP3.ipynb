{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"368fc16d4a7e4a4f8eda1ea1e0b062c5","deepnote_cell_type":"markdown"},"source":["# TP 3 : Adversarial Games, Self-Play, Monte Carlo Tree Search"]},{"cell_type":"markdown","metadata":{"cell_id":"4b7a78d6766d40ec803a52e3ccd8c346","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["By Diyun Lu and Gwendal Debaussart"]},{"cell_type":"markdown","metadata":{"cell_id":"3cee8e1d710f48ffad9d46792d142e6f","deepnote_cell_type":"markdown"},"source":["In this assignement, we will focus on algorithms that play games.\n","\n","You will be evaluated on:\n","* Answering the questions. Bonus points will be given to exploratory answers.\n","* Implementation of the MCTS. Bonus points will be given to clean, scalable code.\n","\n","Send this notebook in a html format to cyriaque.rousselot(at)inria(dot)fr before December 15th.\n","\n","Good Luck !"]},{"cell_type":"markdown","metadata":{"cell_id":"c8426a85ba83433c87e71faba7a23cf4","deepnote_cell_type":"markdown"},"source":["## Tic-tac-toe"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"b199493884dd46ce84600516c5abd9f6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2986,"execution_start":1670834090422,"source_hash":"4a3bc6e8","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.9/site-packages (0.21.0)\n","Requirement already satisfied: numpy>=1.18.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym==0.21) (1.23.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/site-packages (from gym==0.21) (2.2.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n","You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym==0.21"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"28640b4c51144726b0d4fed474776dd2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":128,"execution_start":1670834093409,"source_hash":"5a2892b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["  |   |  \n","  |   |  \n","  |   |  \n","\n","Turn : O\n","\n","  |   |  \n","O |   |  \n","  |   |  \n","\n","Turn : X\n","\n"]}],"source":["from environments import Environment_tic_tac_toe\n","tic_env = Environment_tic_tac_toe() # Load an instance of Tic-Tac-Toe\n","tic_env.reset()\n","tic_env.render()\n","current_state, reward, is_done, info = tic_env.act(1) # return current state,reward, is the game done ?, informations\n","tic_env.render()\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"9127eb805d044e9cb6cc0c7585d6ad31","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1670834093544,"source_hash":"6c5a943a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current state : (array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 1)\n","Reward :  0\n","Is the game done False\n","Additional information {'turn': 1, 'invalid_moves': array([False,  True, False, False, False, False, False, False, False])}\n"]}],"source":["print(\"Current state :\" ,current_state) # 2 arrays + 1 boolean: (Player1 marks, Player2 marks), Turn\n","print(\"Reward : \",reward) # 1 if Player1 win; -1  if player2 win, 0 if draw or if the game continues. \n","print(\"Is the game done\",is_done)# True  if the game has ended.\n","print(\"Additional information\",info)#turn: which player turn  (0 is player 1, 1 is player 2), invalid_moves: occupied squares"]},{"cell_type":"markdown","metadata":{"cell_id":"5799b38b58e64a648f06098a315ebf0c","deepnote_cell_type":"markdown"},"source":["Q.1 Simulate a full game of tic-tac-toe "]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"d999e6919b194daba51d143c14134f25","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1670834093548,"source_hash":"4aa235f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["X |   |  \n","  |   |  \n","  |   |  \n","\n","Turn : O\n","\n","X |   |  \n","O |   |  \n","  |   |  \n","\n","Turn : X\n","\n","X |   |  \n","O |   |  \n","X |   |  \n","\n","Turn : O\n","\n","X | O |  \n","O |   |  \n","X |   |  \n","\n","Turn : X\n","\n","X | O |  \n","O | X |  \n","X |   |  \n","\n","Turn : O\n","\n","X | O |  \n","O | X |  \n","X | O |  \n","\n","Turn : X\n","\n","X | O | X\n","O | X |  \n","X | O |  \n","\n","End of the game \n","\n"]}],"source":["tic_env.reset()\n","i=0\n","current_state, reward, is_done, info = tic_env.act(0) \n","##Simularing a game (with a stupid strategy)\n","while not is_done :\n","    i+=1\n","    tic_env.render()\n","    current_state, reward, is_done, info = tic_env.act(i) \n","    \n","\n","tic_env.render()"]},{"cell_type":"markdown","metadata":{"cell_id":"ad2f1e2b93da471ab229bbdd1b65efa7","deepnote_cell_type":"markdown"},"source":["Q.2 Provide a reasonable upper-bound of the size of the state space. Why is it only an upper-bound ? Give a detailed answer"]},{"cell_type":"markdown","metadata":{"cell_id":"a3c5aae97b3a4fbd94d98665835aee35","deepnote_cell_type":"markdown"},"source":["$3^9 \\times 2$ seems to be a reasonable upper bound : we have 3 action per square and there is 2 senarios : either cross starts or round starts. It's only a upper bound because some cases will never happen in the game, for example:\n","<pre>\n","  |   |           |   |   \n","O | O | O  and  O | O | O \n","X | X | X       X |   |\n","</pre>\n","are situations that will never happen."]},{"cell_type":"markdown","metadata":{"cell_id":"41287c38004741929ef2344def99af22","deepnote_cell_type":"markdown"},"source":["## Competition of agents"]},{"cell_type":"markdown","metadata":{"cell_id":"9bbb3290a94a4292b2d484296008dd12","deepnote_cell_type":"markdown"},"source":["We introduce two agents adapted from previous practicals to play the game of Tic-Tac-Toe. A ```RandomAgent``` that will play random feasable moves and a ```Q_learning_Agent``` that will update its knowledge of the environement.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"bba92bb0a33b4297bb48c27f2d35d4ee","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670834093574,"source_hash":"f3bdb7e"},"outputs":[],"source":["import numpy as np\n","\n","class RandomAgent: # A random agent\n","    def __init__(self,environement):\n","        self.environement = environement\n","        self.n_a = environement.n_a # number of possible actions\n","        self.training = False\n","        \"\"\"Init a new agent.\n","        \"\"\"\n","\n","    def memory(self): # Essential for self play\n","        return None\n","    def choose(self):\n","        return(self.environement.uniform_random_action())\n","\n","\n","    def update(self, observation,action,reward,info):\n","        \"\"\"Receive a reward for performing given action on\n","        given observation.\n","\n","        This is where your agent can learn.\n","        \"\"\"\n","        "]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"fe4df2ce78134941b38d9bbe3fed1760","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670834093574,"source_hash":"3633d9ba"},"outputs":[],"source":["# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","import random\n","class Q_learning_Agent: # A modified version of Q-learning \n","    \n","    def __init__(self,environement,memory=None):\n","        self.environement = environement\n","        self.n_a = environement.n_a\n","        if memory is not None:\n","            self.q_table = memory\n","        else:\n","            self.q_table = np.zeros([self.environement.env.n_states, self.n_a])\n","        self.current = 0\n","        self.training = True\n","        self.random_a=0\n","        self.n_a_cho = 0\n","        \n","    def get_memory(self):\n","        return np.copy(self.q_table)\n","        \n","    def choose(self):\n","        self.n_a_cho+=1\n","        if self.training: # When in training mode\n","            if random.uniform(0, 1) < epsilon:\n","                self.random_a+=1\n","                action = self.environement.uniform_random_action() # Explore action space\n","                return action\n","            else:\n","                action = np.argmax(self.q_table[self.current]) # Exploit learned values\n","                if self.environement.is_invalid(action):\n","                    self.q_table[self.current,action] = -np.inf # Remove the impossible action\n","                    self.random_a+=1\n","                    action = self.environement.uniform_random_action()\n","            return action\n","        \n","        else : # Outside of training mode \n","            action=np.argmax(self.q_table[self.current])\n","            counter_action = 1\n","\n","            if random.uniform(0, 1) < epsilon:\n","                self.random_a+=1\n","                action = self.environement.uniform_random_action()\n","                return action\n","\n","            while self.environement.is_invalid(action):\n","                counter_action += 1\n","                self.q_table[self.current,action] = -np.inf\n","                if counter_action > self.n_a:\n","                    self.random_a+=1\n","                    action = self.environement.uniform_random_action()\n","                else:\n","                    action=np.argmax(self.q_table[self.current])\n","            return action\n","\n","    def reset_random_counter(self):\n","        \"Reset the counters of actions and random actions\"\n","        self.random_a = 0\n","        self.n_a_cho = 0\n","    \n","    def get_random_counter(self):\n","        \"Return the number of time the algorithm chose an action randomly\"\n","        return (self.random_a,self.n_a_cho)\n","\n","    def update(self, observation,action,reward,info):\n","        \"\"\"Receive a reward for performing given action on\n","        given observation.\n","gamma\n","        This is where your agent can learn.\n","        \"\"\"\n","        if self.training:\n","            old_value = self.q_table[self.current, action]\n","            next_max = np.max(self.q_table[obs_to_int(observation)])\n","            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","            self.q_table[self.current, action] = new_value\n","        self.current = obs_to_int(observation)\n","\n","def obs_to_int(obs): # Convert our state representation  into an integer\n","    array,turn =obs\n","    \n","    b = array.flatten()\n","    return int((b.dot(2**np.arange(b.size)[::-1]+1)+1)*(turn+1))    "]},{"cell_type":"markdown","metadata":{"cell_id":"30dc1e61c0b745c08f4ef1dc9673ab31","deepnote_cell_type":"markdown"},"source":["Q (BONUS) Suggest a better method for the q_table implementation."]},{"cell_type":"markdown","metadata":{"cell_id":"f428b757af254536a974a54b26484dd0","deepnote_cell_type":"markdown"},"source":["implementation ... TODO"]},{"cell_type":"markdown","metadata":{"cell_id":"d4d7b68b897f468188ddb9e031ae96bf","deepnote_cell_type":"markdown"},"source":["We will now use a ```Runner``` object to train our agent against himself : The reward seen by the agent when playing as player 2 will be the opposite of the effective reward. \n"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"eea8ff14dc094ae2b66c6c72b8ac3fba","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":21,"execution_start":1670834093575,"source_hash":"a832e9cd"},"outputs":[],"source":["from runner import Runner,Runner_eval_vs,Runner_train_vs\n","from environments import Environment_tic_tac_toe,Environment_go\n","\n","my_env = Environment_tic_tac_toe()\n","my_agent_random = RandomAgent(my_env)\n","my_agent_q = Q_learning_Agent(my_env)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"a9c7246fe56d49eaa4047cd5517c2b9b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":14,"execution_start":1670834093600,"source_hash":"b6032031"},"outputs":[{"name":"stdout","output_type":"stream","text":["  |   |  \n","  |   |  \n","  |   |  \n","\n","Turn : O\n","\n"]}],"source":["my_env.reset()# Reset the environement\n","\n","runner_rd = Runner(my_env,my_agent_random,verbose=True)# Verbose is set to true to generate a display string with the full match\n","list_reward_random,display = runner_rd.loop(5) # List reward : the rewards  we get at the end of the games,\n","my_env.render()                                                #    display: text rendering of the games"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"85a608da86d2439d8436a98cf86e9a47","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670834093659,"source_hash":"81ef097c"},"outputs":[],"source":["with open(\"rapport_partie_rd.txt\",'w', encoding = 'utf-8') as f: # Write the games into a txt file\n","    f.write(display)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"c8560b9210294ea29c10885c8e29e520","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":23295,"execution_start":1670834093659,"source_hash":"7c3033cb"},"outputs":[],"source":["# Training the Q-agent against itself. \n","runner_q = Runner(my_env,my_agent_q,False,training=True)\n","list_reward_q,_ = runner_q.loop(10000) # Can take a while"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"104188dcd37c4eeebbe00f2abd681af7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":981,"execution_start":1670834116957,"source_hash":"af8bc81e"},"outputs":[{"data":{"text/plain":["<BarContainer object of 100 artists>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAr7UlEQVR4nO3df1RVZb7H8c8B5CAVoCEcKAx/rdSbv4KR0O7NGc4S0jWj93YruzQo13BlMmU0qcykpqZYeR2zcYZbo6krHZ3mptOvSzEYdhsJDKOylMlJ08QDJQNHtFBh3z9mdeokGBpbPI/v11p7xXn2dz/7eZ5UPmufvc9xWJZlCQAAwCBBXT0AAACAzkbAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYJ6SrB9AVWltbVVNToyuuuEIOh6OrhwMAADrAsiwdO3ZM8fHxCgo6+zWaSzLg1NTUKCEhoauHAQAAzsOhQ4d09dVXn7Xmkgw4V1xxhaR/LFBEREQXjwYAAHSE1+tVQkKC7/f42VySAeert6UiIiIIOAAABJiO3F7CTcYAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDi2Bpw33nhDP/7xjxUfHy+Hw6GtW7d+5zGlpaW6/vrr5XQ61b9/f61du/aMmlWrVikxMVFhYWFKSUlRRUVF5w8eAAAELFsDzvHjxzVs2DCtWrWqQ/X79+/X+PHj9cMf/lBVVVWaOXOm7rrrLr366qu+ms2bNysvL0/z58/Xrl27NGzYMKWnp6uurs6uaQAAgADjsCzLuiAncji0ZcsWTZw4sd2a2bNn6+WXX9bu3bt9bZMmTVJDQ4OKiookSSkpKfrBD36gX//615Kk1tZWJSQk6Gc/+5nmzJnTobF4vV5FRkaqsbGR76ICACBAnMvv74vqHpyysjK53W6/tvT0dJWVlUmSTp48qcrKSr+aoKAgud1uX01bmpub5fV6/TYAAGCui+rbxD0ej2JjY/3aYmNj5fV69cUXX+jvf/+7Wlpa2qzZu3dvu/0WFBRowYIFtoy5qyXOedn384Gl47twJP/wzfFIbY/p2zXfdrHNo6Pj6cjcL2Q/XX0uE+dxMfzZ7IiOjPlCzqur/yxcbP8Pvz2eQPjzGwj/bn/bRXUFxy75+flqbGz0bYcOHerqIQEAABtdVFdwXC6Xamtr/dpqa2sVERGh7t27Kzg4WMHBwW3WuFyudvt1Op1yOp22jBkAAFx8LqorOKmpqSopKfFrKy4uVmpqqiQpNDRUSUlJfjWtra0qKSnx1QAAANgacJqamlRVVaWqqipJ/3gMvKqqSgcPHpT0j7eOsrKyfPV33323Pv74Y82aNUt79+7Vb37zG/3hD3/Q/fff76vJy8vT008/rXXr1mnPnj2aPn26jh8/ruzsbDunAgAAAoitb1G9/fbb+uEPf+h7nZeXJ0maPHmy1q5dqyNHjvjCjiT16dNHL7/8su6//3498cQTuvrqq/W73/1O6enpvprbb79dn332mebNmyePx6Phw4erqKjojBuPAQDApcvWgDNmzBid7WN22vqU4jFjxuidd945a7+5ubnKzc39vsMDAACGuqjuwQEAAOgMBBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAONckICzatUqJSYmKiwsTCkpKaqoqGi3dsyYMXI4HGds48eP99VMmTLljP0ZGRkXYioAACAAhNh9gs2bNysvL0+FhYVKSUnRihUrlJ6erurqasXExJxR//zzz+vkyZO+10ePHtWwYcN06623+tVlZGTomWee8b12Op32TQIAAAQU26/gLF++XDk5OcrOztbgwYNVWFio8PBwrVmzps36nj17yuVy+bbi4mKFh4efEXCcTqdfXY8ePeyeCgAACBC2BpyTJ0+qsrJSbrf76xMGBcntdqusrKxDfaxevVqTJk3SZZdd5tdeWlqqmJgYXXvttZo+fbqOHj3abh/Nzc3yer1+GwAAMJetAefzzz9XS0uLYmNj/dpjY2Pl8Xi+8/iKigrt3r1bd911l197RkaG1q9fr5KSEj366KPavn27br75ZrW0tLTZT0FBgSIjI31bQkLC+U8KAABc9Gy/B+f7WL16tYYMGaKRI0f6tU+aNMn385AhQzR06FD169dPpaWlSktLO6Of/Px85eXl+V57vV5CDgAABrP1Ck50dLSCg4NVW1vr115bWyuXy3XWY48fP65NmzZp6tSp33mevn37Kjo6Wvv27Wtzv9PpVEREhN8GAADMZWvACQ0NVVJSkkpKSnxtra2tKikpUWpq6lmPfe6559Tc3Kw777zzO8/z6aef6ujRo4qLi/veYwYAAIHP9qeo8vLy9PTTT2vdunXas2ePpk+fruPHjys7O1uSlJWVpfz8/DOOW716tSZOnKgrr7zSr72pqUkPPvig3nrrLR04cEAlJSWaMGGC+vfvr/T0dLunAwAAAoDt9+Dcfvvt+uyzzzRv3jx5PB4NHz5cRUVFvhuPDx48qKAg/5xVXV2tN998U6+99toZ/QUHB+u9997TunXr1NDQoPj4eI0dO1aLFi3is3AAAICkC3STcW5urnJzc9vcV1paekbbtddeK8uy2qzv3r27Xn311c4cHgAAMAzfRQUAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMM4FCTirVq1SYmKiwsLClJKSooqKinZr165dK4fD4beFhYX51ViWpXnz5ikuLk7du3eX2+3WRx99ZPc0AABAgLA94GzevFl5eXmaP3++du3apWHDhik9PV11dXXtHhMREaEjR474tk8++cRv/2OPPaaVK1eqsLBQ5eXluuyyy5Senq4vv/zS7ukAAIAAYHvAWb58uXJycpSdna3BgwersLBQ4eHhWrNmTbvHOBwOuVwu3xYbG+vbZ1mWVqxYoYceekgTJkzQ0KFDtX79etXU1Gjr1q12TwcAAAQAWwPOyZMnVVlZKbfb/fUJg4LkdrtVVlbW7nFNTU265pprlJCQoAkTJuiDDz7w7du/f788Ho9fn5GRkUpJSWm3z+bmZnm9Xr8NAACYy9aA8/nnn6ulpcXvCowkxcbGyuPxtHnMtddeqzVr1uhPf/qTnn32WbW2tmrUqFH69NNPJcl33Ln0WVBQoMjISN+WkJDwfacGAAAuYhfdU1SpqanKysrS8OHDddNNN+n5559Xr1699N///d/n3Wd+fr4aGxt926FDhzpxxAAA4GJja8CJjo5WcHCwamtr/dpra2vlcrk61Ee3bt00YsQI7du3T5J8x51Ln06nUxEREX4bAAAwl60BJzQ0VElJSSopKfG1tba2qqSkRKmpqR3qo6WlRe+//77i4uIkSX369JHL5fLr0+v1qry8vMN9AgAAs4XYfYK8vDxNnjxZycnJGjlypFasWKHjx48rOztbkpSVlaWrrrpKBQUFkqSFCxfqhhtuUP/+/dXQ0KDHH39cn3zyie666y5J/3jCaubMmXrkkUc0YMAA9enTR3PnzlV8fLwmTpxo93QAAEAAsD3g3H777frss880b948eTweDR8+XEVFRb6bhA8ePKigoK8vJP39739XTk6OPB6PevTooaSkJO3YsUODBw/21cyaNUvHjx/XtGnT1NDQoBtvvFFFRUVnfCAgAAC4NNkecCQpNzdXubm5be4rLS31e/2rX/1Kv/rVr87an8Ph0MKFC7Vw4cLOGiIAADDIRfcUFQAAwPdFwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMM4FCTirVq1SYmKiwsLClJKSooqKinZrn376af3zP/+zevTooR49esjtdp9RP2XKFDkcDr8tIyPD7mkAAIAAYXvA2bx5s/Ly8jR//nzt2rVLw4YNU3p6uurq6tqsLy0t1R133KHXX39dZWVlSkhI0NixY3X48GG/uoyMDB05csS3/f73v7d7KgAAIEDYHnCWL1+unJwcZWdna/DgwSosLFR4eLjWrFnTZv2GDRt0zz33aPjw4Ro4cKB+97vfqbW1VSUlJX51TqdTLpfLt/Xo0cPuqQAAgABha8A5efKkKisr5Xa7vz5hUJDcbrfKyso61MeJEyd06tQp9ezZ06+9tLRUMTExuvbaazV9+nQdPXq03T6am5vl9Xr9NgAAYC5bA87nn3+ulpYWxcbG+rXHxsbK4/F0qI/Zs2crPj7eLyRlZGRo/fr1Kikp0aOPPqrt27fr5ptvVktLS5t9FBQUKDIy0rclJCSc/6QAAMBFL6SrB3A2S5cu1aZNm1RaWqqwsDBf+6RJk3w/DxkyREOHDlW/fv1UWlqqtLS0M/rJz89XXl6e77XX6yXkAABgMFuv4ERHRys4OFi1tbV+7bW1tXK5XGc9dtmyZVq6dKlee+01DR069Ky1ffv2VXR0tPbt29fmfqfTqYiICL8NAACYy9aAExoaqqSkJL8bhL+6YTg1NbXd4x577DEtWrRIRUVFSk5O/s7zfPrppzp69Kji4uI6ZdwAACCw2f4UVV5enp5++mmtW7dOe/bs0fTp03X8+HFlZ2dLkrKyspSfn++rf/TRRzV37lytWbNGiYmJ8ng88ng8ampqkiQ1NTXpwQcf1FtvvaUDBw6opKREEyZMUP/+/ZWenm73dAAAQACw/R6c22+/XZ999pnmzZsnj8ej4cOHq6ioyHfj8cGDBxUU9HXO+u1vf6uTJ0/q3//93/36mT9/vh5++GEFBwfrvffe07p169TQ0KD4+HiNHTtWixYtktPptHs6AAAgAFyQm4xzc3OVm5vb5r7S0lK/1wcOHDhrX927d9err77aSSMDAAAm4ruoAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGuSABZ9WqVUpMTFRYWJhSUlJUUVFx1vrnnntOAwcOVFhYmIYMGaJXXnnFb79lWZo3b57i4uLUvXt3ud1uffTRR3ZOAQAABBDbA87mzZuVl5en+fPna9euXRo2bJjS09NVV1fXZv2OHTt0xx13aOrUqXrnnXc0ceJETZw4Ubt37/bVPPbYY1q5cqUKCwtVXl6uyy67TOnp6fryyy/tng4AAAgAtgec5cuXKycnR9nZ2Ro8eLAKCwsVHh6uNWvWtFn/xBNPKCMjQw8++KAGDRqkRYsW6frrr9evf/1rSf+4erNixQo99NBDmjBhgoYOHar169erpqZGW7dutXs6AAAgANgacE6ePKnKykq53e6vTxgUJLfbrbKysjaPKSsr86uXpPT0dF/9/v375fF4/GoiIyOVkpLSbp/Nzc3yer1+GwAAMJfDsizLrs5ramp01VVXaceOHUpNTfW1z5o1S9u3b1d5efkZx4SGhmrdunW64447fG2/+c1vtGDBAtXW1mrHjh0aPXq0ampqFBcX56u57bbb5HA4tHnz5jP6fPjhh7VgwYIz2hsbGxUREfF9p9kpEue87Pf6wNLxF925vnmcneO70DqyHhfy/09Xu9j+P7P2ndvvufR9PuPpzL9P3z5/W8ddzH9eL4bxXEgX4u+q1+tVZGRkh35/XxJPUeXn56uxsdG3HTp0qKuHBAAAbGRrwImOjlZwcLBqa2v92mtra+Vyudo8xuVynbX+q/+eS59Op1MRERF+GwAAMJetASc0NFRJSUkqKSnxtbW2tqqkpMTvLatvSk1N9auXpOLiYl99nz595HK5/Gq8Xq/Ky8vb7RMAAFxaQuw+QV5eniZPnqzk5GSNHDlSK1as0PHjx5WdnS1JysrK0lVXXaWCggJJ0n333aebbrpJ//Vf/6Xx48dr06ZNevvtt/XUU09JkhwOh2bOnKlHHnlEAwYMUJ8+fTR37lzFx8dr4sSJdk8HAAAEANsDzu23367PPvtM8+bNk8fj0fDhw1VUVKTY2FhJ0sGDBxUU9PWFpFGjRmnjxo166KGH9Itf/EIDBgzQ1q1bdd111/lqZs2apePHj2vatGlqaGjQjTfeqKKiIoWFhdk9HQAAEABsDziSlJubq9zc3Db3lZaWntF266236tZbb223P4fDoYULF2rhwoWdNUQAAGCQS+IpKgAAcGkh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGMfWgFNfX6/MzExFREQoKipKU6dOVVNT01nrf/azn+naa69V9+7d1bt3b917771qbGz0q3M4HGdsmzZtsnMqAAAggITY2XlmZqaOHDmi4uJinTp1StnZ2Zo2bZo2btzYZn1NTY1qamq0bNkyDR48WJ988onuvvtu1dTU6I9//KNf7TPPPKOMjAzf66ioKDunAgAAAohtAWfPnj0qKirSzp07lZycLEl68sknNW7cOC1btkzx8fFnHHPdddfpf/7nf3yv+/Xrp8WLF+vOO+/U6dOnFRLy9XCjoqLkcrnsGj4AAAhgtr1FVVZWpqioKF+4kSS3262goCCVl5d3uJ/GxkZFRET4hRtJmjFjhqKjozVy5EitWbNGlmW120dzc7O8Xq/fBgAAzGXbFRyPx6OYmBj/k4WEqGfPnvJ4PB3q4/PPP9eiRYs0bdo0v/aFCxfqRz/6kcLDw/Xaa6/pnnvuUVNTk+699942+ykoKNCCBQvObyIAACDgnPMVnDlz5rR5k+83t717937vgXm9Xo0fP16DBw/Www8/7Ldv7ty5Gj16tEaMGKHZs2dr1qxZevzxx9vtKz8/X42Njb7t0KFD33t8AADg4nXOV3AeeOABTZky5aw1ffv2lcvlUl1dnV/76dOnVV9f/533zhw7dkwZGRm64oortGXLFnXr1u2s9SkpKVq0aJGam5vldDrP2O90OttsBwAAZjrngNOrVy/16tXrO+tSU1PV0NCgyspKJSUlSZK2bdum1tZWpaSktHuc1+tVenq6nE6nXnjhBYWFhX3nuaqqqtSjRw9CDAAAkGTjPTiDBg1SRkaGcnJyVFhYqFOnTik3N1eTJk3yPUF1+PBhpaWlaf369Ro5cqS8Xq/Gjh2rEydO6Nlnn/W7IbhXr14KDg7Wiy++qNraWt1www0KCwtTcXGxlixZop///Od2TQUAAAQYWz8HZ8OGDcrNzVVaWpqCgoJ0yy23aOXKlb79p06dUnV1tU6cOCFJ2rVrl+8Jq/79+/v1tX//fiUmJqpbt25atWqV7r//flmWpf79+2v58uXKycmxcyoAACCA2Bpwevbs2e6H+klSYmKi3+PdY8aMOevj3pKUkZHh9wF/AAAA38Z3UQEAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjGNrwKmvr1dmZqYiIiIUFRWlqVOnqqmp6azHjBkzRg6Hw2+7++67/WoOHjyo8ePHKzw8XDExMXrwwQd1+vRpO6cCAAACSIidnWdmZurIkSMqLi7WqVOnlJ2drWnTpmnjxo1nPS4nJ0cLFy70vQ4PD/f93NLSovHjx8vlcmnHjh06cuSIsrKy1K1bNy1ZssS2uQAAgMBhW8DZs2ePioqKtHPnTiUnJ0uSnnzySY0bN07Lli1TfHx8u8eGh4fL5XK1ue+1117Thx9+qD//+c+KjY3V8OHDtWjRIs2ePVsPP/ywQkNDbZkPAAAIHLa9RVVWVqaoqChfuJEkt9utoKAglZeXn/XYDRs2KDo6Wtddd53y8/N14sQJv36HDBmi2NhYX1t6erq8Xq8++OCDNvtrbm6W1+v12wAAgLlsu4Lj8XgUExPjf7KQEPXs2VMej6fd4/7jP/5D11xzjeLj4/Xee+9p9uzZqq6u1vPPP+/r95vhRpLvdXv9FhQUaMGCBd9nOgAAIICcc8CZM2eOHn300bPW7Nmz57wHNG3aNN/PQ4YMUVxcnNLS0vS3v/1N/fr1O68+8/PzlZeX53vt9XqVkJBw3mMEAAAXt3MOOA888ICmTJly1pq+ffvK5XKprq7Or/306dOqr69v9/6atqSkpEiS9u3bp379+snlcqmiosKvpra2VpLa7dfpdMrpdHb4nAAAILCdc8Dp1auXevXq9Z11qampamhoUGVlpZKSkiRJ27ZtU2trqy+0dERVVZUkKS4uztfv4sWLVVdX53sLrLi4WBERERo8ePA5zgYAAJjItpuMBw0apIyMDOXk5KiiokJ/+ctflJubq0mTJvmeoDp8+LAGDhzouyLzt7/9TYsWLVJlZaUOHDigF154QVlZWfqXf/kXDR06VJI0duxYDR48WD/96U/17rvv6tVXX9VDDz2kGTNmcJUGAABIsvmD/jZs2KCBAwcqLS1N48aN04033qinnnrKt//UqVOqrq72PSUVGhqqP//5zxo7dqwGDhyoBx54QLfccotefPFF3zHBwcF66aWXFBwcrNTUVN15553Kysry+9wcAABwabP1g/569ux51g/1S0xMlGVZvtcJCQnavn37d/Z7zTXX6JVXXumUMQIAAPPwXVQAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOPYGnDq6+uVmZmpiIgIRUVFaerUqWpqamq3/sCBA3I4HG1uzz33nK+urf2bNm2ycyoAACCAhNjZeWZmpo4cOaLi4mKdOnVK2dnZmjZtmjZu3NhmfUJCgo4cOeLX9tRTT+nxxx/XzTff7Nf+zDPPKCMjw/c6Kiqq08cPAAACk20BZ8+ePSoqKtLOnTuVnJwsSXryySc1btw4LVu2TPHx8WccExwcLJfL5de2ZcsW3Xbbbbr88sv92qOios6oBQAAkGx8i6qsrExRUVG+cCNJbrdbQUFBKi8v71AflZWVqqqq0tSpU8/YN2PGDEVHR2vkyJFas2aNLMtqt5/m5mZ5vV6/DQAAmMu2Kzgej0cxMTH+JwsJUc+ePeXxeDrUx+rVqzVo0CCNGjXKr33hwoX60Y9+pPDwcL322mu655571NTUpHvvvbfNfgoKCrRgwYLzmwgAAAg453wFZ86cOe3eCPzVtnfv3u89sC+++EIbN25s8+rN3LlzNXr0aI0YMUKzZ8/WrFmz9Pjjj7fbV35+vhobG33boUOHvvf4AADAxeucr+A88MADmjJlyllr+vbtK5fLpbq6Or/206dPq76+vkP3zvzxj3/UiRMnlJWV9Z21KSkpWrRokZqbm+V0Os/Y73Q622wHAABmOueA06tXL/Xq1es761JTU9XQ0KDKykolJSVJkrZt26bW1lalpKR85/GrV6/WT37ykw6dq6qqSj169CDEAAAASTbegzNo0CBlZGQoJydHhYWFOnXqlHJzczVp0iTfE1SHDx9WWlqa1q9fr5EjR/qO3bdvn9544w298sorZ/T74osvqra2VjfccIPCwsJUXFysJUuW6Oc//7ldUwEAAAHG1s/B2bBhg3Jzc5WWlqagoCDdcsstWrlypW//qVOnVF1drRMnTvgdt2bNGl199dUaO3bsGX1269ZNq1at0v333y/LstS/f38tX75cOTk5dk4FAAAEEFsDTs+ePdv9UD9JSkxMbPPx7iVLlmjJkiVtHpORkeH3AX8AAADfxndRAQAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADAOAQcAABiHgAMAAIxDwAEAAMYh4AAAAOMQcAAAgHEIOAAAwDgEHAAAYBwCDgAAMA4BBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMY1vAWbx4sUaNGqXw8HBFRUV16BjLsjRv3jzFxcWpe/fucrvd+uijj/xq6uvrlZmZqYiICEVFRWnq1KlqamqyYQYAACBQ2RZwTp48qVtvvVXTp0/v8DGPPfaYVq5cqcLCQpWXl+uyyy5Tenq6vvzyS19NZmamPvjgAxUXF+ull17SG2+8oWnTptkxBQAAEKBC7Op4wYIFkqS1a9d2qN6yLK1YsUIPPfSQJkyYIElav369YmNjtXXrVk2aNEl79uxRUVGRdu7cqeTkZEnSk08+qXHjxmnZsmWKj4+3ZS4AACCwXDT34Ozfv18ej0dut9vXFhkZqZSUFJWVlUmSysrKFBUV5Qs3kuR2uxUUFKTy8vJ2+25ubpbX6/XbAACAuRyWZVl2nmDt2rWaOXOmGhoazlq3Y8cOjR49WjU1NYqLi/O133bbbXI4HNq8ebOWLFmidevWqbq62u/YmJgYLViwoN23wx5++GHfFaVvamxsVERExLlPCgAAXHBer1eRkZEd+v19Tldw5syZI4fDcdZt796932vwdsjPz1djY6NvO3ToUFcPCQAA2Oic7sF54IEHNGXKlLPW9O3b97wG4nK5JEm1tbV+V3Bqa2s1fPhwX01dXZ3fcadPn1Z9fb3v+LY4nU45nc7zGhcAAAg85xRwevXqpV69etkykD59+sjlcqmkpMQXaLxer8rLy31vPaWmpqqhoUGVlZVKSkqSJG3btk2tra1KSUmxZVwAACDw2HaT8cGDB1VVVaWDBw+qpaVFVVVVqqqq8vvMmoEDB2rLli2SJIfDoZkzZ+qRRx7RCy+8oPfff19ZWVmKj4/XxIkTJUmDBg1SRkaGcnJyVFFRob/85S/Kzc3VpEmTeIIKAAD42PaY+Lx587Ru3Trf6xEjRkiSXn/9dY0ZM0aSVF1drcbGRl/NrFmzdPz4cU2bNk0NDQ268cYbVVRUpLCwMF/Nhg0blJubq7S0NAUFBemWW27RypUr7ZoGAAAIQLY/RXUxOpe7sAEAwMXBtqeoAAAAAgEBBwAAGIeAAwAAjEPAAQAAxiHgAAAA4xBwAACAcQg4AADAOAQcAABgHAIOAAAwjm1f1XAx++rDm71ebxePBAAAdNRXv7c78iUMl2TAOXbsmCQpISGhi0cCAADO1bFjxxQZGXnWmkvyu6haW1tVU1OjK664Qg6Ho9P793q9SkhI0KFDh/iuK5ux1hcG63zhsNYXBut8YXT2OluWpWPHjik+Pl5BQWe/y+aSvIITFBSkq6++2vbzRERE8BfnAmGtLwzW+cJhrS8M1vnC6Mx1/q4rN1/hJmMAAGAcAg4AADAOAccGTqdT8+fPl9Pp7OqhGI+1vjBY5wuHtb4wWOcLoyvX+ZK8yRgAAJiNKzgAAMA4BBwAAGAcAg4AADAOAQcAABiHgGODVatWKTExUWFhYUpJSVFFRUVXDymgFRQU6Ac/+IGuuOIKxcTEaOLEiaqurvar+fLLLzVjxgxdeeWVuvzyy3XLLbeotra2i0ZshqVLl8rhcGjmzJm+Nta58xw+fFh33nmnrrzySnXv3l1DhgzR22+/7dtvWZbmzZunuLg4de/eXW63Wx999FEXjjjwtLS0aO7cuerTp4+6d++ufv36adGiRX7fY8Q6n5833nhDP/7xjxUfHy+Hw6GtW7f67e/IutbX1yszM1MRERGKiorS1KlT1dTU1HmDtNCpNm3aZIWGhlpr1qyxPvjgAysnJ8eKioqyamtru3poASs9Pd165plnrN27d1tVVVXWuHHjrN69e1tNTU2+mrvvvttKSEiwSkpKrLffftu64YYbrFGjRnXhqANbRUWFlZiYaA0dOtS67777fO2sc+eor6+3rrnmGmvKlClWeXm59fHHH1uvvvqqtW/fPl/N0qVLrcjISGvr1q3Wu+++a/3kJz+x+vTpY33xxRddOPLAsnjxYuvKK6+0XnrpJWv//v3Wc889Z11++eXWE0884athnc/PK6+8Yv3yl7+0nn/+eUuStWXLFr/9HVnXjIwMa9iwYdZbb71l/d///Z/Vv39/64477ui0MRJwOtnIkSOtGTNm+F63tLRY8fHxVkFBQReOyix1dXWWJGv79u2WZVlWQ0OD1a1bN+u5557z1ezZs8eSZJWVlXXVMAPWsWPHrAEDBljFxcXWTTfd5As4rHPnmT17tnXjjTe2u7+1tdVyuVzW448/7mtraGiwnE6n9fvf//5CDNEI48ePt/7zP//Tr+3f/u3frMzMTMuyWOfO8u2A05F1/fDDDy1J1s6dO301//u//2s5HA7r8OHDnTIu3qLqRCdPnlRlZaXcbrevLSgoSG63W2VlZV04MrM0NjZKknr27ClJqqys1KlTp/zWfeDAgerduzfrfh5mzJih8ePH+62nxDp3phdeeEHJycm69dZbFRMToxEjRujpp5/27d+/f788Ho/fWkdGRiolJYW1PgejRo1SSUmJ/vrXv0qS3n33Xb355pu6+eabJbHOdunIupaVlSkqKkrJycm+GrfbraCgIJWXl3fKOC7JL9u0y+eff66WlhbFxsb6tcfGxmrv3r1dNCqztLa2aubMmRo9erSuu+46SZLH41FoaKiioqL8amNjY+XxeLpglIFr06ZN2rVrl3bu3HnGPta583z88cf67W9/q7y8PP3iF7/Qzp07de+99yo0NFSTJ0/2rWdb/5aw1h03Z84ceb1eDRw4UMHBwWppadHixYuVmZkpSayzTTqyrh6PRzExMX77Q0JC1LNnz05bewIOAsqMGTO0e/duvfnmm109FOMcOnRI9913n4qLixUWFtbVwzFaa2urkpOTtWTJEknSiBEjtHv3bhUWFmry5MldPDpz/OEPf9CGDRu0ceNG/dM//ZOqqqo0c+ZMxcfHs86XAN6i6kTR0dEKDg4+46mS2tpauVyuLhqVOXJzc/XSSy/p9ddf19VXX+1rd7lcOnnypBoaGvzqWfdzU1lZqbq6Ol1//fUKCQlRSEiItm/frpUrVyokJESxsbGscyeJi4vT4MGD/doGDRqkgwcPSpJvPfm35Pt58MEHNWfOHE2aNElDhgzRT3/6U91///0qKCiQxDrbpSPr6nK5VFdX57f/9OnTqq+v77S1J+B0otDQUCUlJamkpMTX1traqpKSEqWmpnbhyAKbZVnKzc3Vli1btG3bNvXp08dvf1JSkrp16+a37tXV1Tp48CDrfg7S0tL0/vvvq6qqyrclJycrMzPT9zPr3DlGjx59xkcd/PWvf9U111wjSerTp49cLpffWnu9XpWXl7PW5+DEiRMKCvL/NRccHKzW1lZJrLNdOrKuqampamhoUGVlpa9m27Ztam1tVUpKSucMpFNuVYbPpk2bLKfTaa1du9b68MMPrWnTpllRUVGWx+Pp6qEFrOnTp1uRkZFWaWmpdeTIEd924sQJX83dd99t9e7d29q2bZv19ttvW6mpqVZqamoXjtoM33yKyrJY585SUVFhhYSEWIsXL7Y++ugja8OGDVZ4eLj17LPP+mqWLl1qRUVFWX/605+s9957z5owYQKPL5+jyZMnW1dddZXvMfHnn3/eio6OtmbNmuWrYZ3Pz7Fjx6x33nnHeueddyxJ1vLly6133nnH+uSTTyzL6ti6ZmRkWCNGjLDKy8utN9980xowYACPiV/snnzySat3795WaGioNXLkSOutt97q6iEFNEltbs8884yv5osvvrDuueceq0ePHlZ4eLj1r//6r9aRI0e6btCG+HbAYZ07z4svvmhdd911ltPptAYOHGg99dRTfvtbW1utuXPnWrGxsZbT6bTS0tKs6urqLhptYPJ6vdZ9991n9e7d2woLC7P69u1r/fKXv7Sam5t9Nazz+Xn99dfb/Hd58uTJlmV1bF2PHj1q3XHHHdbll19uRUREWNnZ2daxY8c6bYwOy/rGRzoCAAAYgHtwAACAcQg4AADAOAQcAABgHAIOAAAwDgEHAAAYh4ADAACMQ8ABAADGIeAAAADjEHAAAIBxCDgAAMA4BBwAAGAcAg4AADDO/wOsUeTROxNefwAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{"image/png":{"height":413,"width":568}},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt \n","plt.bar(range(len(list_reward_q[-100:])),list_reward_q[-100:])"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"767d1fbdda1a4f12aef217172b2925c7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":25,"execution_start":1670834117943,"source_hash":"d4aac7dc"},"outputs":[],"source":["# Visualize Q-learning Agent games\n","my_env.reset()\n","\n","runner_q_d = Runner(my_env,my_agent_q,True,training=False)# Verbose is set to true to generate a display string with the full match\n","_,display = runner_q_d.loop(10)"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"2111a8078de448e6baa87e224fbe9503","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1670834117970,"source_hash":"2a8f6b03"},"outputs":[],"source":["with open(\"rapport_partie_q.txt\",'w', encoding = 'utf-8') as f: # Write the games into a txt file\n","    f.write(display)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"17936c2fbeff4d209b433843525dcfeb","deepnote_cell_type":"markdown"},"source":["Q.3 Compare the games of the Random agent and of the Q-learning agent."]},{"cell_type":"markdown","metadata":{"cell_id":"d561a3ced09446c69985f9848775c37a","deepnote_cell_type":"markdown"},"source":["The random agent's game seems to end more with either of one player winning the game, wheras the game with the Q-Agent seems to end up often with a tie"]},{"cell_type":"markdown","metadata":{"cell_id":"8ae7167499e540c6a5ad1a1d545832c6","deepnote_cell_type":"markdown"},"source":["Q.4 What can you say about the Q-Learning agent games ? Is the Agent performing well ?"]},{"cell_type":"markdown","metadata":{"cell_id":"e44f64f77d1d42de86549f20a196ca87","deepnote_cell_type":"markdown"},"source":["The Q-Agent game seems to end with ties, the agent seems to perform well enough against itself"]},{"cell_type":"markdown","metadata":{"cell_id":"e98dea1c379c4494b46d2ed422578ae9","deepnote_cell_type":"markdown"},"source":["## Evaluate our Q-Agent against a random Agent"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"6a0707dac191446b81f908bdb155db07","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1591,"execution_start":1670834117992,"source_hash":"6a046a52"},"outputs":[{"name":"stdout","output_type":"stream","text":["383 4006\n"]}],"source":["my_agent_q.reset_random_counter()\n","eval_runner = Runner_eval_vs(my_env,my_agent_q,my_agent_random) # Evaluate the agent my_agent_q against the random agent my_agent_random\n","list_match_q_rd,_ = eval_runner.loop(1000)\n","random_n,n = my_agent_q.get_random_counter()\n","print(random_n,n)"]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":"27efb920f2f54470a17dcefe8ef8001a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2687,"execution_start":1670834119626,"source_hash":"4433e1d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["838 8882\n"]}],"source":["tic_env.reset()\n","my_agent_q.reset_random_counter()\n","eval_runner = Runner(my_env,my_agent_q,Q_learning_Agent(tic_env,my_agent_q.get_memory())) # Evaluate the agent my_agent_q against the random agent my_agent_random\n","list_match_q_rd,_ = eval_runner.loop(1000)\n","random_n,n = my_agent_q.get_random_counter()\n","print(random_n,n)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"cell_id":"2e1293ae05de43ed8f714bfeb984aa88","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670834122324,"source_hash":"ae647f66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Percentage of win of Agent Q :  0.111\n","Number of win of Random Agent  :  0.108\n","Number of null :  0.781\n"]}],"source":["# Counting the wins,losses and draw of the first evaluated agent\n","list_match= np.array(list_match_q_rd)\n","count_agent_q = np.count_nonzero(list_match == 1.0)\n","count_agent_rd = np.count_nonzero(list_match == -1.0)\n","count_null = np.count_nonzero(list_match == 0.0)\n","total = np.shape(list_match)[0]\n","\n","print(\"Percentage of win of Agent Q : \",count_agent_q/total)\n","print(\"Number of win of Random Agent  : \",count_agent_rd/total)\n","print(\"Number of null : \",count_null/total)"]},{"cell_type":"code","execution_count":17,"metadata":{"cell_id":"bd32bf2ad1c34376b5823360ffe4c145","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2737,"execution_start":1670834122329,"source_hash":"8a9aaf70"},"outputs":[{"name":"stdout","output_type":"stream","text":["894 8866\n"]}],"source":["my_agent_q.reset_random_counter()\n","eval_runner = Runner(my_env,my_agent_q,Q_learning_Agent(tic_env,memory=my_agent_q.get_memory())) # Evaluate the agent my_agent_q against a copy of itself\n","list_match_q_q,_ = eval_runner.loop(1000)\n","random_n,n = my_agent_q.get_random_counter()\n","print(random_n,n)"]},{"cell_type":"markdown","metadata":{"cell_id":"3cb6074a00984d939f4b58e4dee9acb0","deepnote_cell_type":"markdown"},"source":["Q.5 Using the ```get_random_counter(), reset_random_counter() ``` method to count the number of random decisions of the Q_learning agent, what can you say about the decisions of the Q_learning agent against himself ? Against a Random Agent ? Why ?"]},{"cell_type":"markdown","metadata":{"cell_id":"cd5b210311354da1ad59b1c5c8d473c7","deepnote_cell_type":"markdown"},"source":["it would seem that the Q-Agent plays more randomly against it-self than against a random Agent. It's surely due to the fact that when the Q-agent is against itself it has multiple possibility of \"good choice\" or possibility that would lead to a tie, where as against a random agent, the good actions may be a bit more rare. Also, the Q-Learning don't know in advance whqt the random agent will do."]},{"cell_type":"markdown","metadata":{"cell_id":"acba37e1ebb2490fa5e8229b7e7758ed","deepnote_cell_type":"markdown"},"source":["Q.6 Conclude on why  our adapted Q_learning agent is performing poorly against a random agent"]},{"cell_type":"markdown","metadata":{"cell_id":"24a1cb9feb0347dab4c455fc1b9c0a84","deepnote_cell_type":"markdown"},"source":["Because the Q-Learning was trained against itself, thus it learnt to play good against it's own strategy and not against any strategy."]},{"cell_type":"markdown","metadata":{"cell_id":"cde3ab274191436cb37d3b5951983d46","deepnote_cell_type":"markdown"},"source":["Q.7 Suggest a method to improve the performance of the Q-learning agent. Bonus points will be given to implementation."]},{"cell_type":"markdown","metadata":{"cell_id":"47f3e0acece44386bc6405e3a516323a","deepnote_cell_type":"markdown"},"source":["Maybe by training the Q-Agent in competition with other agent could help it perform better"]},{"cell_type":"markdown","metadata":{"cell_id":"b8f20bc4a488468f856c365684a5a66c","deepnote_cell_type":"markdown"},"source":["## Monte Carlo Tree Search"]},{"cell_type":"markdown","metadata":{"cell_id":"a5e3ffd6f05e49c79f0d35fa0da9d17d","deepnote_cell_type":"markdown"},"source":["https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\n","\n","The MCTS algorithm we will implement can be divided in 4 steps:\n","- Selection \n","- Expansion\n","- Simulation\n","- Backpropagation\n","\n","The first step is exploring the current tree using a UCB rule until we get to a leaf L .\n","\n","The second is creating a child C from feasable moves after the leaf L if the game is not finished.\n","\n","The third is simulating the end of the game with an unbiased method to get an estimate of the value of the position C.\n","\n","The fourth is updating the value estimation of the position of all nodes visited during the exploration of the tree.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"197d72e3c64a4c948037960983685c60","deepnote_cell_type":"markdown"},"source":["Points will be granted even if the end algorithms doesn't run."]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"217e812b16d040fe918db0e93f03ac24","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1670834125073,"source_hash":"6beef90c"},"outputs":[],"source":["from environments import Environment_tic_tac_toe\n","\n","tic_env = Environment_tic_tac_toe()"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"fac7ddce676a4245abf8d4bad0703ec6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8,"execution_start":1670834125084,"source_hash":"f2c41fcb"},"outputs":[],"source":["import numpy as np\n","def obs_to_int(obs): # Convert our binary representation of arrays into an integer\n","    array,turn =obs\n","    \n","    b = array.flatten()\n","    return int((b.dot(2**np.arange(b.size)[::-1]+1)+1)*(turn+1))     "]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"7ceff18514ce440a85f3862a478ed881","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670834125102,"source_hash":"f4a9acbd"},"outputs":[],"source":["#Building the Tree Structure Recursively\n","\n","class Node:\n","    def __init__(self,observation, parent=None, action=None):\n","        self.parent = parent # Which Node is the parent\n","        self.action = action # What action is associated to this Node\n","        self.state = obs_to_int(observation) # What state is associated to this Node\n","        self.children = [] # What are the children Nodes of this Node\n","        self.explored_children = 0 # The number of children we explored\n","        self.n_visits = 0 # Tje amount of time we have visited this Node\n","        self.results = {-1:0,0:0,1:0} \n","    def value(self):\n","        return self.results[1]-self.results[-1]\n","    def __str__(self):\n","        return \"Node(\"+'c: ['+\" \".join([i.__str__() for i in self.children])+']'+', a:'+str(self.action)+', s:'+str(self.state)+\")\""]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"fe162904a4c744cd84cbb4d8f53bc62b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":19,"execution_start":1670834125121,"source_hash":"d1d8d2ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Tree : Node(c: [], a:None, s:2)\n","  |   |  \n","  |   |  \n","X |   |  \n","\n","Turn : O\n","\n","Current Tree : Node(c: [Node(c: [], a:2, s:66)], a:None, s:2)\n","  |   |  \n","  |   |  \n","X | O |  \n","\n","Turn : X\n","\n","Current Tree : Node(c: [Node(c: [Node(c: [], a:5, s:8326)], a:2, s:66)], a:None, s:2)\n"]}],"source":["#Observing the building of the tree\n","\n","obs = tic_env.reset()\n","Root = Node(observation=obs) # We convert every array representation of state into an integer.\n","print(\"Initial Tree :\",Root)\n","action_0=2\n","\n","obs, reward, done, info = tic_env.act(action_0)\n","Node_action_0 = Node(obs,Root,action_0)\n","Root.children.append(Node_action_0)\n","tic_env.render()\n","print(\"Current Tree :\",Root)\n","\n","action_1=5\n","obs, reward, done, info = tic_env.act(action_1)\n","Leaf_action_1 = Node(obs,Node_action_0,action_1)\n","tic_env.render()\n","Node_action_0.children.append(Leaf_action_1)\n","print(\"Current Tree :\",Root)"]},{"cell_type":"markdown","metadata":{"cell_id":"7b9129835db64d319e5517bd15e7dd80","deepnote_cell_type":"markdown"},"source":["Q.8 Using the UCB valuation, build a function selection that select the best possible child among children of a node"]},{"cell_type":"code","execution_count":22,"metadata":{"cell_id":"f949ec8eb73e48c5aefb7a8e2c1426e1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670834125140,"source_hash":"f53d1f66"},"outputs":[],"source":["def ucb(node,c=0.1):\n","    return node.value() / node.n_visits + np.sqrt(c*np.log(node.parent.n_visits)/node.n_visits)\n","\n","# def selection(node):\n","#     selected_child = np.argmax(ucb(node.children)) # TO COMPLETE\n","#     return selected_child\n","\n","def selection(node):\n","    # calculate the ucb values for each children nodes.\n","    ucb_values = []\n","    for child in node.children:\n","        ucb_value = ucb(child)\n","        ucb_values.append(ucb_value)\n","\n","    selected_child = node.children[np.argmax(ucb_values)]\n","    return selected_child\n"]},{"cell_type":"markdown","metadata":{"cell_id":"ee3ff1390c8c48f3a5d1b28066c87c95","deepnote_cell_type":"markdown"},"source":["Q.9 Build an Expansion function that generates a new child from feasable actions, add it to the children of the node. You may use the ``` invalid_moves()``` method from the Environment class. You will generate the new child along running the environement. You can't generate one child if there is already an existing child with the same associated action. "]},{"cell_type":"code","execution_count":23,"metadata":{"cell_id":"d1db4b30c6574f30900fe5826f6edfeb","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670834125141,"source_hash":"872dfa3c"},"outputs":[],"source":["def expansion(node,env):\n","    new_child = node\n","    # possible_actions = [env.uniform_random_action() for _ in range(env.n_a)]\n","    valid_actions = [index for index, a in enumerate(my_env.invalid_moves()) if a == False]\n","    # check if the action already exists\n","    is_existed = True\n","    for action in valid_actions:\n","        is_existed = False\n","        for child in node.children:\n","            if child.action == action:\n","                is_existed = True\n","                break\n","    # if it doesn't exist, then we get the next observation\n","    if not is_existed:\n","        next_observation, reward, done, info = env.act(action)\n","        # and we can create a new child node\n","        new_child = Node(next_observation, parent=node, action=action)\n","        node.children.append(new_child)\n","    \n","    return new_child"]},{"cell_type":"markdown","metadata":{"cell_id":"45d39743781c47d58568f7a382ed6adb","deepnote_cell_type":"markdown"},"source":["Q.10 Build a simulation function (rollout) that simulate the rest of the game until the end. It will return the outcome of the game.  You may use the ```uniform_random_action(),is_done(),act()``` method from the Environment class. You have to compare the finishing turn to the starting turn ```env.init_turn``` to give a proper reward to the end game."]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"fc8d6c052124496a9cf0a4f8049c9bc1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1670834125145,"source_hash":"c1d62d5b"},"outputs":[],"source":["def simulation(node,env):\n","    reward = 0\n","    # if the game is not over, then do the simulation\n","    while not env.is_done():\n","        # choose an action\n","        action = env.uniform_random_action()\n","        # run the action\n","        env.act(action)\n","    # compute the reward\n","    if env.init_turn == env.turn():\n","        reward = 1\n","    else:\n","        reward = -1\n","    return reward"]},{"cell_type":"markdown","metadata":{"cell_id":"0ce8b52acb2a41f9b6a85592dffcaa14","deepnote_cell_type":"markdown"},"source":["Q.11 Build a backpropagation function that update for the  node visited the number of visits, and the number of losses, draw and win "]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"6bd5aee16b5444cc91e8f93032ee9799","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1670834125155,"source_hash":"9928767f"},"outputs":[],"source":["def backpropagation(node:Node,result):\n","    # if current node has father, then continue the recursion\n","    if node.parent:\n","        backpropagation(node.parent,result)\n","    # update the visited numbers\n","    node.n_visits += 1\n","    # update the results\n","    node.results[result] += 1"]},{"cell_type":"markdown","metadata":{"cell_id":"b8f97d3a81e8476d80c1844b5ab345df","deepnote_cell_type":"markdown"},"source":["Q.12 Build a function that follows the tree and return a leaf. While the game is not finished, the function verify if all possible children of the current node exists(looking at feasable moves), and select the best child from them"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"db38e4919f054d2b9b637e9795458420","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":28,"execution_start":1670834125166,"source_hash":"e40f8fd2"},"outputs":[],"source":["def in_the_tree_policy(node:Node,env): \n","    current_node = node\n","    while not env.is_done():\n","        # if all the possible children nodes are existed of current node, then we will choose the best child.\n","        possible_actions = [env.uniform_random_action() for _ in range(env.n_a)]\n","        valid_actions = [a for a in possible_actions if a not in env.invalid_moves()]\n","        # possible_actions = [action_to_function(action) for action in actions]\n","        if len(current_node.children) == len(valid_actions):\n","            current_node = selection(current_node)\n","        # Otherwise, we expand current node and return the new node\n","        else:\n","            return expansion(current_node, env)\n","    return current_node\n"]},{"cell_type":"markdown","metadata":{"cell_id":"c7ad6337b8924f70b670298a6a90fbe8","deepnote_cell_type":"markdown"},"source":["Q.13 Build a function choose_action that perform rollouts from a node,  find a leaf in the tree using ```in_the_tree_policy()```,simulate its result and backpropagate after ```n_rollouts``` iterations. You may need to use the ```deepcopy``` function to pass a copy of the environement for the roll-out."]},{"cell_type":"code","execution_count":38,"metadata":{"cell_id":"1869bbb22b864fba88d2e349b8d532cf","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670835364095,"source_hash":"8ab2916"},"outputs":[],"source":["from copy import deepcopy\n","def choose_action(node:Node,env):\n","    n_rollouts = 200 # Number of rollout to perform before backpropagation (originally 100)\n","\t\n","    for i in range(n_rollouts):\n","        rolling_env = deepcopy(env)\n","        leaf = in_the_tree_policy(node, rolling_env)\n","        reward = simulation(leaf, rolling_env)\n","        backpropagation(leaf, reward)\n","    return selection(node).action"]},{"cell_type":"markdown","metadata":{"cell_id":"d3aa3b58dddd4e0da91c4c080799bc5d","deepnote_cell_type":"markdown"},"source":["Q.14 Finalize your MCTS algorithm and launch it on the Tic Tac Toe Environement."]},{"cell_type":"code","execution_count":39,"metadata":{"cell_id":"84dc0d8e093f4d44add93963a5ceb9bd","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1670835364095,"source_hash":"566fdba2"},"outputs":[],"source":["from copy import deepcopy\n","class MCTS:\n","    def __init__(self,environement,memory=None):\n","        self.tree = Node(environement.reset())\n","        self.n_a = environement.n_a\n","        self.environement = environement\n","        self.current_node = self.tree\n","        self.training = False\n","\n","    def get_memory(self):\n","        return deepcopy(self.tree)\n","        \n","    def choose(self): # Select the next good action\n","        action = choose_action(self.current_node,self.environement)\n","        return action\n","\n","    def update(self, observation,action,reward,info):\n","        # find the children node that is associated with current node\n","        next_node = None\n","        for child in self.current_node.children:\n","            if child.action == action:\n","                next_node = child\n","                break\n","        # if the next_node is not existed, then we create a new node\n","        if not next_node:\n","            next_node = Node(observation, parent=self.current_node, action=action)\n","            self.current_node.children.append(next_node)\n","        # update current node\n","        self.current_node = next_node\n","        \n","    \n","my_MCTS = MCTS(tic_env)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"dd2565f2e2614a1c9f2075f44f2a7207","deepnote_cell_type":"markdown"},"source":["Q.15 Evaluate your MCTS against other policies"]},{"cell_type":"code","execution_count":42,"metadata":{"cell_id":"3a03540b9900492683888027219b1498","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":461688,"execution_start":1670835815416,"source_hash":"e29c935f","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Percentage of win of Agent MCTS :  0.55\n","Number of win of Random Agent  :  0.43\n","Number of null :  0.02\n","Number of win of Random Q  :  0.16\n"]}],"source":["eval_runner = Runner_eval_vs(my_env,my_MCTS,my_agent_random) # Evaluate the agent my_MCTS against the random agent my_agent_random\n","list_match_MCTS_rd,_ = eval_runner.loop(100)\n","random_n,n = my_agent_q.get_random_counter()\n","\n","tic_env.reset()\n","eval_runner = Runner(my_env,my_agent_q,False,training=False) # Evaluate the agent my_agent_q against the random agent my_agent_random\n","#eval_runner = Runner_eval_vs(my_env,my_agent_q,my_agent_random)\n","list_match_q_rd,_ = eval_runner.loop(100)\n","random_n,n = my_agent_q.get_random_counter()\n","\n","# Counting the wins,losses and draw of the first evaluated agent\n","list_match= np.array(list_match_MCTS_rd)\n","count_agent_MCTS = np.count_nonzero(list_match == 1.0)\n","count_agent_rd = np.count_nonzero(list_match == -1.0)\n","count_null = np.count_nonzero(list_match == 0.0)\n","total = np.shape(list_match)[0]\n","\n","print(\"Percentage of win of Agent MCTS : \",count_agent_MCTS/total)\n","print(\"Number of win of Random Agent  : \",count_agent_rd/total)\n","print(\"Number of null : \",count_null/total)\n","\n","list_match_q= np.array(list_match_q_rd)\n","count_agent_q = np.count_nonzero(list_match_q == 1.0)\n","total = np.shape(list_match_q)[0]\n","print(\"Number of win of Random Q  : \", count_agent_q/total)"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"43ea31d8c4b145eeb09a904b19b41fef","kernelspec":{"display_name":"Python 3.9.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3e9516067f19d14f3fb38b8032255053869a23452b6f6538e58e9a0d8d3170fd"}}},"nbformat":4,"nbformat_minor":0}
